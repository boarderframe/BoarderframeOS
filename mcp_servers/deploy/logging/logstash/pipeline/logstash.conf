input {
  # Beats input for log shipping from applications
  beats {
    port => 5044
  }
  
  # TCP input for application logs
  tcp {
    port => 5000
    codec => json_lines
  }
  
  # HTTP input for webhook logs
  http {
    port => 8080
    codec => json
  }
  
  # Kubernetes logs via Filebeat
  beats {
    port => 5045
    type => "kubernetes"
  }
}

filter {
  # Parse container logs from Kubernetes
  if [kubernetes] {
    # Parse JSON logs if they exist
    if [message] =~ /^\{.*\}$/ {
      json {
        source => "message"
        target => "app"
      }
      
      # Remove the original message field if JSON parsing was successful
      if [app] {
        mutate {
          remove_field => ["message"]
        }
      }
    }
    
    # Add Kubernetes metadata
    mutate {
      add_field => {
        "source" => "kubernetes"
        "environment" => "%{[kubernetes][labels][environment]}"
        "service" => "%{[kubernetes][labels][app]}"
        "pod_name" => "%{[kubernetes][pod][name]}"
        "namespace" => "%{[kubernetes][namespace]}"
        "container_name" => "%{[kubernetes][container][name]}"
        "node_name" => "%{[kubernetes][node][name]}"
      }
    }
  }
  
  # Parse MCP Manager application logs
  if [fields][service] == "mcp-manager" or [service] == "mcp-manager" {
    # Parse structured JSON logs
    if [message] =~ /^\{.*\}$/ {
      json {
        source => "message"
        target => "app_log"
      }
    }
    
    # Parse FastAPI access logs
    grok {
      match => { 
        "message" => [
          "%{IPORHOST:client_ip} - - \[%{HTTPDATE:timestamp}\] \"%{WORD:method} %{URIPATH:path}(?:%{URIPARAM:params})? HTTP/%{NUMBER:http_version}\" %{INT:status} %{INT:bytes} \"%{DATA:referrer}\" \"%{DATA:user_agent}\" %{NUMBER:response_time:float}",
          "%{TIMESTAMP_ISO8601:timestamp} \| %{LOGLEVEL:level} \| %{DATA:logger} \| %{GREEDYDATA:message}"
        ]
      }
      tag_on_failure => ["_grokparsefailure_mcp_manager"]
    }
    
    # Convert response time to float
    if [response_time] {
      mutate {
        convert => { "response_time" => "float" }
      }
    }
    
    # Parse status codes
    if [status] {
      mutate {
        convert => { "status" => "integer" }
      }
      
      # Categorize status codes
      if [status] >= 200 and [status] < 300 {
        mutate { add_field => { "status_category" => "success" } }
      } else if [status] >= 300 and [status] < 400 {
        mutate { add_field => { "status_category" => "redirect" } }
      } else if [status] >= 400 and [status] < 500 {
        mutate { add_field => { "status_category" => "client_error" } }
      } else if [status] >= 500 {
        mutate { add_field => { "status_category" => "server_error" } }
      }
    }
    
    # Extract user ID from logs if present
    if [app_log][user_id] {
      mutate {
        add_field => { "user_id" => "%{[app_log][user_id]}" }
      }
    }
    
    # Extract request ID for tracing
    if [app_log][request_id] {
      mutate {
        add_field => { "request_id" => "%{[app_log][request_id]}" }
      }
    }
  }
  
  # Parse PostgreSQL logs
  if [fields][service] == "postgresql" or [service] == "postgresql" {
    grok {
      match => { 
        "message" => [
          "%{TIMESTAMP_ISO8601:timestamp} \[%{INT:pid}\]: \[%{INT:line}-1\] user=%{USER:db_user},db=%{DATA:database},app=%{DATA:application},client=%{IPORHOST:client_ip} %{LOGLEVEL:level}: %{GREEDYDATA:sql_message}",
          "%{TIMESTAMP_ISO8601:timestamp} \[%{INT:pid}\]: \[%{INT:line}-2\] user=%{USER:db_user},db=%{DATA:database},app=%{DATA:application},client=%{IPORHOST:client_ip} STATEMENT: %{GREEDYDATA:sql_statement}"
        ]
      }
      tag_on_failure => ["_grokparsefailure_postgresql"]
    }
    
    # Parse duration from PostgreSQL logs
    if [sql_message] =~ /duration: [\d\.]+/ {
      grok {
        match => { "sql_message" => "duration: %{NUMBER:query_duration:float}" }
      }
    }
  }
  
  # Parse Redis logs
  if [fields][service] == "redis" or [service] == "redis" {
    grok {
      match => { 
        "message" => [
          "%{INT:pid}:%{WORD:role} %{TIMESTAMP_ISO8601:timestamp} %{CHAR:level} %{GREEDYDATA:redis_message}"
        ]
      }
      tag_on_failure => ["_grokparsefailure_redis"]
    }
  }
  
  # Parse Nginx access logs
  if [fields][service] == "nginx" or [service] == "nginx" {
    grok {
      match => { 
        "message" => "%{COMBINEDAPACHELOG}"
      }
      tag_on_failure => ["_grokparsefailure_nginx"]
    }
    
    # Parse response time from Nginx
    if [message] =~ /request_time/ {
      grok {
        match => { 
          "message" => "request_time:%{NUMBER:request_time:float}"
        }
      }
    }
  }
  
  # Common field parsing and enrichment
  
  # Parse timestamp field if it exists
  if [timestamp] {
    date {
      match => [ "timestamp", "ISO8601", "dd/MMM/yyyy:HH:mm:ss Z", "yyyy-MM-dd HH:mm:ss.SSS" ]
      target => "@timestamp"
    }
  }
  
  # Normalize log level
  if [level] {
    mutate {
      uppercase => [ "level" ]
    }
  }
  
  # Add geographic information for client IPs
  if [client_ip] and [client_ip] !~ /^(10\.|172\.(1[6-9]|2[0-9]|3[01])\.|192\.168\.|127\.)/ {
    geoip {
      source => "client_ip"
      target => "geoip"
    }
  }
  
  # Anonymize sensitive data
  if [sql_statement] {
    # Anonymize potential passwords in SQL
    mutate {
      gsub => [
        "sql_statement", "password\s*=\s*'[^']+'", "password='***'",
        "sql_statement", "password\s*=\s*\"[^\"]+\"", "password=\"***\"",
        "sql_statement", "token\s*=\s*'[^']+'", "token='***'",
        "sql_statement", "token\s*=\s*\"[^\"]+\"", "token=\"***\""
      ]
    }
  }
  
  # Add environment and service tags
  mutate {
    add_field => {
      "environment" => "${ENVIRONMENT:production}"
      "cluster" => "${CLUSTER_NAME:mcp-ui-production}"
      "log_type" => "application"
    }
  }
  
  # Remove unnecessary fields to reduce storage
  mutate {
    remove_field => [
      "beat",
      "input",
      "prospector",
      "offset",
      "source",
      "fields"
    ]
  }
  
  # Error detection and classification
  if [level] == "ERROR" or [status] >= 500 {
    mutate {
      add_tag => ["error"]
    }
    
    # Classify error types
    if [sql_message] =~ /deadlock/ {
      mutate { add_tag => ["deadlock"] }
    } else if [sql_message] =~ /connection/ {
      mutate { add_tag => ["connection_error"] }
    } else if [message] =~ /timeout/ {
      mutate { add_tag => ["timeout"] }
    } else if [message] =~ /memory|OOM/ {
      mutate { add_tag => ["memory_error"] }
    }
  }
  
  # Security event detection
  if [status] == 401 {
    mutate { add_tag => ["authentication_failure"] }
  } else if [status] == 403 {
    mutate { add_tag => ["authorization_failure"] }
  } else if [user_agent] =~ /bot|crawler|spider/i {
    mutate { add_tag => ["bot_traffic"] }
  }
  
  # Performance monitoring
  if [response_time] {
    if [response_time] > 5.0 {
      mutate { add_tag => ["slow_response"] }
    } else if [response_time] > 2.0 {
      mutate { add_tag => ["moderate_response"] }
    }
  }
  
  if [query_duration] {
    if [query_duration] > 1000.0 {
      mutate { add_tag => ["slow_query"] }
    } else if [query_duration] > 500.0 {
      mutate { add_tag => ["moderate_query"] }
    }
  }
}

output {
  # Main Elasticsearch output
  elasticsearch {
    hosts => ["${ES_HOSTS:elasticsearch:9200}"]
    user => "${ELASTIC_USER:elastic}"
    password => "${ELASTIC_PASSWORD}"
    index => "mcp-ui-logs-%{+YYYY.MM.dd}"
    template_name => "mcp-ui-logs"
    template_pattern => "mcp-ui-logs-*"
    template => "/usr/share/logstash/templates/mcp-ui-template.json"
    template_overwrite => true
  }
  
  # Separate index for error logs
  if "error" in [tags] {
    elasticsearch {
      hosts => ["${ES_HOSTS:elasticsearch:9200}"]
      user => "${ELASTIC_USER:elastic}"
      password => "${ELASTIC_PASSWORD}"
      index => "mcp-ui-errors-%{+YYYY.MM.dd}"
    }
  }
  
  # Separate index for security events
  if "authentication_failure" in [tags] or "authorization_failure" in [tags] {
    elasticsearch {
      hosts => ["${ES_HOSTS:elasticsearch:9200}"]
      user => "${ELASTIC_USER:elastic}"
      password => "${ELASTIC_PASSWORD}"
      index => "mcp-ui-security-%{+YYYY.MM.dd}"
    }
  }
  
  # Output to file for debugging (only in non-production)
  if "${ENVIRONMENT:production}" != "production" {
    file {
      path => "/var/log/logstash/debug-%{+YYYY-MM-dd}.log"
      codec => json_lines
    }
  }
  
  # Metrics output for monitoring Logstash itself
  statsd {
    host => "statsd"
    increment => [
      "logstash.events.processed",
      "logstash.events.%{[service]}.processed"
    ]
    gauge => {
      "logstash.events.processing_time" => "%{[processing_time]}"
    }
  }
}